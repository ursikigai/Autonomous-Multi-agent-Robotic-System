#!/usr/bin/env python3
# env_from_states.py
"""
A simple Gymnasium-compatible environment that reads precomputed per-frame
state JSON files (generated by generate_states.py) and exposes a vector
observation + discrete action space for training with Stable-Baselines3.

Observation:
 - Flattened occupancy patch (size = P)
 - robot_x, robot_y, robot_z (3)
 - nearest_object_distance (1)
 - num_objects (1)
 -> final observation vector length = P + 5

Action space (example):
 - 0: move forward
 - 1: turn left
 - 2: turn right
 - 3: slow down / stop

Reward (simple baseline):
 - +1 if nearest_object_distance increases (safer)
 - -5 if collision (distance < 1.0m)
 - small shaping reward for forward progress (optional)
"""

import os, json, glob, random
import numpy as np
import gymnasium as gym
from gymnasium import spaces

class StatesDatasetEnv(gym.Env):
    metadata = {"render.modes": ["rgb_array", "human"]}

    def __init__(self, states_dir, max_episode_len=200, patch_len=None):
        super().__init__()
        files = sorted(glob.glob(os.path.join(states_dir, "frame_*.json")))
        if len(files) == 0:
            raise RuntimeError("No state JSONs found in " + states_dir)
        self.files = files
        self.max_episode_len = max_episode_len
        self.cur_idx = 0
        self.episode_step = 0

        # read one to infer sizes
        sample = json.load(open(files[0]))
        patch_len = patch_len or len(sample["occupancy_patch"])
        obs_len = patch_len + 5  # patch + (x,y,z) + nearest_dist + num_objs

        self.observation_space = spaces.Box(low=-1e6, high=1e6, shape=(obs_len,), dtype=np.float32)
        self.action_space = spaces.Discrete(4)

        # parameters for reward shaping
        self.collision_thresh = 1.0

    def _load_state(self, idx):
        data = json.load(open(self.files[idx]))
        # occupancy patch normalization (0..10 values -> scale to 0..1)
        patch = np.array(data["occupancy_patch"], dtype=np.float32)
        patch = patch / (patch.max() + 1e-6)
        rx, ry, rz = data["robot_pose"]
        nearest = data["nearest_object_distance"]
        num_objs = float(data["num_objects"])
        obs = np.concatenate([patch, np.array([rx, ry, rz, nearest, num_objs], dtype=np.float32)])
        return obs, data

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        # start episodes at random frame to diversify
        self.cur_idx = random.randint(0, max(0, len(self.files) - 1 - self.max_episode_len))
        self.episode_step = 0
        obs, info = self._load_state(self.cur_idx)
        return obs, {}

    def step(self, action):
        # This is an env built on pre-recorded frames. We simulate outcome
        # by moving to next frame and computing reward from nearest distance change.
        prev_obs, prev_info = self._load_state(self.cur_idx)
        prev_nearest = prev_info["nearest_object_distance"] if prev_info["nearest_object_distance"] is not None else 1e6

        # action effect is illustrative. You can extend to modify "virtual" robot pose
        # For now, action only affects reward shaping minimally (encourage forward action)
        self.cur_idx = min(self.cur_idx + 1, len(self.files) - 1)
        self.episode_step += 1

        obs, info = self._load_state(self.cur_idx)
        nearest = info["nearest_object_distance"] if info["nearest_object_distance"] is not None else 1e6

        # reward: positive if distance to nearest object increased (safer)
        reward = float(nearest - prev_nearest)

        # penalty for collision
        done = False
        if nearest < self.collision_thresh:
            reward -= 5.0
            done = True

        # optional small forward bonus for 'forward' action (action 0)
        if action == 0:
            reward += 0.05

        if self.episode_step >= self.max_episode_len:
            done = True

        info_out = {"frame_idx": self.cur_idx}
        return obs, reward, done, False, info_out

    def render(self, mode="human"):
        return None

    def close(self):
        pass

if __name__ == "__main__":
    # quick smoke test
    env = StatesDatasetEnv("../results/state_vectors", max_episode_len=50)
    o, _ = env.reset()
    print("obs shape:", o.shape)
    for i in range(5):
        a = env.action_space.sample()
        o, r, d, t, info = env.step(a)
        print(i, "r", r, "done", d)
        if d: break
#!/usr/bin/env python3
# env_from_states.py
"""
A simple Gymnasium-compatible environment that reads precomputed per-frame
state JSON files (generated by generate_states.py) and exposes a vector
observation + discrete action space for training with Stable-Baselines3.

Observation:
 - Flattened occupancy patch (size = P)
 - robot_x, robot_y, robot_z (3)
 - nearest_object_distance (1)
 - num_objects (1)
 -> final observation vector length = P + 5

Action space (example):
 - 0: move forward
 - 1: turn left
 - 2: turn right
 - 3: slow down / stop

Reward (simple baseline):
 - +1 if nearest_object_distance increases (safer)
 - -5 if collision (distance < 1.0m)
 - small shaping reward for forward progress (optional)
"""

import os, json, glob, random
import numpy as np
import gymnasium as gym
from gymnasium import spaces

class StatesDatasetEnv(gym.Env):
    metadata = {"render.modes": ["rgb_array", "human"]}

    def __init__(self, states_dir, max_episode_len=200, patch_len=None):
        super().__init__()
        files = sorted(glob.glob(os.path.join(states_dir, "frame_*.json")))
        if len(files) == 0:
            raise RuntimeError("No state JSONs found in " + states_dir)
        self.files = files
        self.max_episode_len = max_episode_len
        self.cur_idx = 0
        self.episode_step = 0

        # read one to infer sizes
        sample = json.load(open(files[0]))
        patch_len = patch_len or len(sample["occupancy_patch"])
        obs_len = patch_len + 5  # patch + (x,y,z) + nearest_dist + num_objs

        self.observation_space = spaces.Box(low=-1e6, high=1e6, shape=(obs_len,), dtype=np.float32)
        self.action_space = spaces.Discrete(4)

        # parameters for reward shaping
        self.collision_thresh = 1.0

    def _load_state(self, idx):
        data = json.load(open(self.files[idx]))
        # occupancy patch normalization (0..10 values -> scale to 0..1)
        patch = np.array(data["occupancy_patch"], dtype=np.float32)
        patch = patch / (patch.max() + 1e-6)
        rx, ry, rz = data["robot_pose"]
        nearest = data["nearest_object_distance"]
        num_objs = float(data["num_objects"])
        obs = np.concatenate([patch, np.array([rx, ry, rz, nearest, num_objs], dtype=np.float32)])
        return obs, data

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        # start episodes at random frame to diversify
        self.cur_idx = random.randint(0, max(0, len(self.files) - 1 - self.max_episode_len))
        self.episode_step = 0
        obs, info = self._load_state(self.cur_idx)
        return obs, {}

    def step(self, action):
        # This is an env built on pre-recorded frames. We simulate outcome
        # by moving to next frame and computing reward from nearest distance change.
        prev_obs, prev_info = self._load_state(self.cur_idx)
        prev_nearest = prev_info["nearest_object_distance"] if prev_info["nearest_object_distance"] is not None else 1e6

        # action effect is illustrative. You can extend to modify "virtual" robot pose
        # For now, action only affects reward shaping minimally (encourage forward action)
        self.cur_idx = min(self.cur_idx + 1, len(self.files) - 1)
        self.episode_step += 1

        obs, info = self._load_state(self.cur_idx)
        nearest = info["nearest_object_distance"] if info["nearest_object_distance"] is not None else 1e6

        # reward: positive if distance to nearest object increased (safer)
        reward = float(nearest - prev_nearest)

        # penalty for collision
        done = False
        if nearest < self.collision_thresh:
            reward -= 5.0
            done = True

        # optional small forward bonus for 'forward' action (action 0)
        if action == 0:
            reward += 0.05

        if self.episode_step >= self.max_episode_len:
            done = True

        info_out = {"frame_idx": self.cur_idx}
        return obs, reward, done, False, info_out

    def render(self, mode="human"):
        return None

    def close(self):
        pass

if __name__ == "__main__":
    # quick smoke test
    env = StatesDatasetEnv("../results/state_vectors", max_episode_len=50)
    o, _ = env.reset()
    print("obs shape:", o.shape)
    for i in range(5):
        a = env.action_space.sample()
        o, r, d, t, info = env.step(a)
        print(i, "r", r, "done", d)
        if d: break
#!/usr/bin/env python3
# train_from_states.py
import os
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from stable_baselines3.common.vec_env import DummyVecEnv
from env_from_states import StatesDatasetEnv

def make_env():
    return StatesDatasetEnv("../results/state_vectors", max_episode_len=200)

env = DummyVecEnv([make_env])

model = PPO("MlpPolicy", env, verbose=1, n_steps=2048, batch_size=64, ent_coef=0.01, learning_rate=3e-4)
chk = CheckpointCallback(save_freq=5000, save_path="./models/", name_prefix="ppo_states")
os.makedirs("models", exist_ok=True)
model.learn(total_timesteps=200_000, callback=chk)
model.save("models/ppo_states_final.zip")
print("Training done and saved to models/ppo_states_final.zip")

